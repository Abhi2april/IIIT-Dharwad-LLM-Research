{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8961335,"sourceType":"datasetVersion","datasetId":5393829},{"sourceId":9286052,"sourceType":"datasetVersion","datasetId":5621124},{"sourceId":4298,"sourceType":"modelInstanceVersion","modelInstanceId":3093,"modelId":735}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installations, imports, utils","metadata":{}},{"cell_type":"code","source":"!pip install transformers==4.33.0 accelerate==0.22.0 einops==0.6.1 langchain==0.0.300 xformers==0.0.21 \\\nbitsandbytes==0.41.1 sentence_transformers==2.2.2 chromadb==0.4.12 --quiet","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-09-08T11:55:57.233647Z","iopub.execute_input":"2024-09-08T11:55:57.234081Z","iopub.status.idle":"2024-09-08T11:58:48.497593Z","shell.execute_reply.started":"2024-09-08T11:55:57.234045Z","shell.execute_reply":"2024-09-08T11:58:48.496410Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-cloud-pubsublite 1.8.2 requires overrides<7.0.0,>=6.0.1, but you have overrides 7.7.0 which is incompatible.\njupyterlab-lsp 4.2.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\ntorchdata 0.6.0 requires torch==2.0.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install rouge-score==0.0.4 nltk==3.8.1 bert-score==0.3.11 --quiet\n\nimport numpy as np\nimport torch\nfrom sklearn.metrics import precision_score, recall_score\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom rouge_score import rouge_scorer\nfrom bert_score import score as bert_score\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\n\nimport nltk\nnltk.download('wordnet')\nnltk.download('omw-1.4')\nfrom nltk.translate.meteor_score import meteor_score\n\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-08T11:59:15.863998Z","iopub.execute_input":"2024-09-08T11:59:15.864837Z","iopub.status.idle":"2024-09-08T11:59:42.136791Z","shell.execute_reply.started":"2024-09-08T11:59:15.864794Z","shell.execute_reply":"2024-09-08T11:59:42.135760Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch import cuda, bfloat16\nimport torch\nimport transformers\nfrom transformers import AutoTokenizer\nfrom time import time\n\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.chains import RetrievalQA\nfrom langchain.vectorstores import Chroma\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-09-08T11:59:59.736806Z","iopub.execute_input":"2024-09-08T11:59:59.737192Z","iopub.status.idle":"2024-09-08T12:00:03.044923Z","shell.execute_reply.started":"2024-09-08T11:59:59.737157Z","shell.execute_reply":"2024-09-08T12:00:03.044108Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Initialize model, tokenizer, query pipeline","metadata":{}},{"cell_type":"markdown","source":"Define the model, the device, and the `bitsandbytes` configuration.","metadata":{}},{"cell_type":"code","source":"model_id = '/kaggle/input/llama-2/pytorch/7b-chat-hf/1'\n\ndevice = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n\n# this requires the `bitsandbytes` library\nbnb_config = transformers.BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=bfloat16\n)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-09-08T12:00:11.296370Z","iopub.execute_input":"2024-09-08T12:00:11.297235Z","iopub.status.idle":"2024-09-08T12:00:11.304319Z","shell.execute_reply.started":"2024-09-08T12:00:11.297204Z","shell.execute_reply":"2024-09-08T12:00:11.303427Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Prepare the model and the tokenizer.","metadata":{}},{"cell_type":"code","source":"time_1 = time()\nmodel_config = transformers.AutoConfig.from_pretrained(\n    model_id,\n)\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n    model_id,\n    trust_remote_code=True,\n    config=model_config,\n    quantization_config=bnb_config,\n    device_map='auto',\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntime_2 = time()\nprint(f\"Prepare model, tokenizer: {round(time_2-time_1, 3)} sec.\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-09-08T12:00:18.951459Z","iopub.execute_input":"2024-09-08T12:00:18.952144Z","iopub.status.idle":"2024-09-08T12:02:42.562986Z","shell.execute_reply.started":"2024-09-08T12:00:18.952113Z","shell.execute_reply":"2024-09-08T12:02:42.561980Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b762d4d5cec47d4a8d3c337b505694a"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Prepare model, tokenizer: 143.605 sec.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Define the query pipeline.","metadata":{}},{"cell_type":"code","source":"time_1 = time()\nquery_pipeline = transformers.pipeline(\n        \"text-generation\",\n        model=model,\n        tokenizer=tokenizer,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",)\ntime_2 = time()\nprint(f\"Prepare pipeline: {round(time_2-time_1, 3)} sec.\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-09-08T12:02:47.526622Z","iopub.execute_input":"2024-09-08T12:02:47.527262Z","iopub.status.idle":"2024-09-08T12:02:49.162342Z","shell.execute_reply.started":"2024-09-08T12:02:47.527231Z","shell.execute_reply":"2024-09-08T12:02:49.161373Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Prepare pipeline: 1.63 sec.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We define a function for testing the pipeline.","metadata":{}},{"cell_type":"code","source":"def test_model(tokenizer, pipeline, prompt_to_test):\n    \"\"\"\n    Perform a query\n    print the result\n    Args:\n        tokenizer: the tokenizer\n        pipeline: the pipeline\n        prompt_to_test: the prompt\n    Returns\n        None\n    \"\"\"\n    # adapted from https://huggingface.co/blog/llama2#using-transformers\n    time_1 = time()\n    sequences = pipeline(\n        prompt_to_test,\n        do_sample=True,\n        top_k=10,\n        num_return_sequences=1,\n        eos_token_id=tokenizer.eos_token_id,\n        max_length=200,)\n    time_2 = time()\n    print(f\"Test inference: {round(time_2-time_1, 3)} sec.\")\n    for seq in sequences:\n        print(f\"Result: {seq['generated_text']}\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-09-08T12:03:02.334307Z","iopub.execute_input":"2024-09-08T12:03:02.335168Z","iopub.status.idle":"2024-09-08T12:03:02.341053Z","shell.execute_reply.started":"2024-09-08T12:03:02.335136Z","shell.execute_reply":"2024-09-08T12:03:02.340037Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Test the query pipeline\n\nWe test the pipeline with a query about the meaning of State of the Union (SOTU).","metadata":{}},{"cell_type":"code","source":"test_model(tokenizer,\n           query_pipeline,\n           \"Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-09-08T12:03:07.663409Z","iopub.execute_input":"2024-09-08T12:03:07.663817Z","iopub.status.idle":"2024-09-08T12:03:18.488751Z","shell.execute_reply.started":"2024-09-08T12:03:07.663788Z","shell.execute_reply":"2024-09-08T12:03:18.487763Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1417: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Test inference: 10.821 sec.\nResult: Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\n\nThe State of the Union address is a yearly speech delivered by the President of the United States to a joint session of Congress, in which the President outlines the current state of the union and highlights significant issues and goals for the coming year. The address is designed to provide a comprehensive overview of the nation's political, economic, and social landscape, and to articulate the President's vision for the future of the country. (100 words)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install langchain_community --quiet","metadata":{"execution":{"iopub.status.busy":"2024-09-08T12:03:39.767366Z","iopub.execute_input":"2024-09-08T12:03:39.767740Z","iopub.status.idle":"2024-09-08T12:03:59.918120Z","shell.execute_reply.started":"2024-09-08T12:03:39.767712Z","shell.execute_reply":"2024-09-08T12:03:59.916861Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\nchex 0.1.82 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\njupyterlab-lsp 4.2.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nmomepy 0.6.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.23.5 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.2 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\ntorchdata 0.6.0 requires torch==2.0.0, but you have torch 2.0.1 which is incompatible.\nydata-profiling 4.3.1 requires scipy<1.11,>=1.4.1, but you have scipy 1.11.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install pymupdf --quiet","metadata":{"execution":{"iopub.status.busy":"2024-09-08T12:04:10.105456Z","iopub.execute_input":"2024-09-08T12:04:10.105855Z","iopub.status.idle":"2024-09-08T12:04:24.287132Z","shell.execute_reply.started":"2024-09-08T12:04:10.105824Z","shell.execute_reply":"2024-09-08T12:04:24.286013Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"from langchain_community.document_loaders import PyMuPDFLoader","metadata":{"execution":{"iopub.status.busy":"2024-09-08T12:04:30.176234Z","iopub.execute_input":"2024-09-08T12:04:30.177185Z","iopub.status.idle":"2024-09-08T12:04:30.379089Z","shell.execute_reply.started":"2024-09-08T12:04:30.177146Z","shell.execute_reply":"2024-09-08T12:04:30.378089Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"loader = PyMuPDFLoader(\"/kaggle/input/lawandorder/Bharatiya_Nyay_(Second)_Sanhita_2023.pdf\")\ndocument1 = loader.load()","metadata":{"execution":{"iopub.status.busy":"2024-09-08T12:04:38.471649Z","iopub.execute_input":"2024-09-08T12:04:38.472322Z","iopub.status.idle":"2024-09-08T12:04:38.901840Z","shell.execute_reply.started":"2024-09-08T12:04:38.472286Z","shell.execute_reply":"2024-09-08T12:04:38.900820Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Split data in chunks\n\nWe split data in chunks using a recursive character text splitter.","metadata":{}},{"cell_type":"code","source":"text_splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=20)\nall_splits = text_splitter.split_documents(documents)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating Embeddings and Storing in Vector Store","metadata":{}},{"cell_type":"markdown","source":"Create the embeddings using Sentence Transformer and HuggingFace embeddings.","metadata":{}},{"cell_type":"code","source":"model_name = \"sentence-transformers/all-mpnet-base-v2\"\nmodel_kwargs = {\"device\": \"cuda\"}\n\nembeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Initialize ChromaDB with the document splits, the embeddings defined previously and with the option to persist it locally.","metadata":{}},{"cell_type":"code","source":"vectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=\"chroma_db\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Initialize chain","metadata":{}},{"cell_type":"code","source":"retriever = vectordb.as_retriever()\n\nqa = RetrievalQA.from_chain_type(\n    llm=llm, \n    chain_type=\"stuff\", \n    retriever=retriever, \n    verbose=True\n)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test the Retrieval-Augmented Generation \n\n\nWe define a test function, that will run the query and time it.","metadata":{}},{"cell_type":"code","source":"def test_rag(qa, query):\n    print(f\"Query: {query}\\n\")\n    time_1 = time()\n    result = qa.run(query)\n    time_2 = time()\n    print(f\"Inference time: {round(time_2-time_1, 3)} sec.\")\n    print(\"\\nResult: \", result)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check few queries.","metadata":{}},{"cell_type":"code","source":"query = \"\"\"\nyou are an intelligent assistant to assist educators to generate curricullam based question on the provideed pdf of class 8th mathematics\nInstructions for Generating Mathematics Problems:-\nTopic Coverage: Ensure that the problems you create span the various topics covered in this chapter.\nOriginality: The questions you generate should not be identical to or minor variations of the questions already present in the PDFs. Instead, use the concepts and examples as a foundation to create entirely new problems.\n\nNow generate 2 mathematics questions.\n\"\"\"\ntest_rag(qa, query)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Document sources\n\nLet's check the documents sources, for the last query run.","metadata":{}},{"cell_type":"markdown","source":"\nWe used Langchain, ChromaDB and Llama 2 as a LLM to build a Retrieval Augmented Generation solution. \nFor IIIT DHARWAD, FOR THE TOPIC OF 'CURRICULAM BASED QUESTION GENERATION'-(FOR EXACT TOPIC AND QUESTION RELATIONAL MATCHES)\n\n\n","metadata":{}},{"cell_type":"markdown","source":"## WE SHALL CLEAR DOWN THE DATABASE MEMORY FOR NEXT BOOK READ","metadata":{}},{"cell_type":"code","source":"import shutil as st\nst.rmtree(\"/kaggle/working/chroma_db\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References  \n\n[1] Murtuza Kazmi, Using LLaMA 2.0, FAISS and LangChain for Question-Answering on Your Own Data, https://medium.com/@murtuza753/using-llama-2-0-faiss-and-langchain-for-question-answering-on-your-own-data-682241488476  \n\n[2] Patrick Lewis, Ethan Perez, et. al., Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, https://browse.arxiv.org/pdf/2005.11401.pdf \n\n[3] Minhajul Hoque, Retrieval Augmented Generation: Grounding AI Responses in Factual Data, https://medium.com/@minh.hoque/retrieval-augmented-generation-grounding-ai-responses-in-factual-data-b7855c059322  \n\n[4] Fangrui Liu\t, Discover the Performance Gain with Retrieval Augmented Generation, https://thenewstack.io/discover-the-performance-gain-with-retrieval-augmented-generation/\n\n[5] Andrew, How to use Retrieval-Augmented Generation (RAG) with Llama 2, https://agi-sphere.com/retrieval-augmented-generation-llama2/   \n\n[6] Yogendra Sisodia, Retrieval Augmented Generation Using Llama2 And Falcon, https://medium.com/@scholarly360/retrieval-augmented-generation-using-llama2-and-falcon-ed26c7b14670   \n\n","metadata":{}}]}